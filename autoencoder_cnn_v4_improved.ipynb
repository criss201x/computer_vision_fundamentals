{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador de Imágenes con Autoencoder CNN - Versión Mejorada\n",
    "\n",
    "## Mejoras implementadas:\n",
    "- ✅ Corrección de bug dimensional en el clasificador\n",
    "- ✅ Data augmentation para mejor generalización\n",
    "- ✅ Validación durante entrenamiento (train/val/test split)\n",
    "- ✅ Early stopping para evitar overfitting\n",
    "- ✅ Batch normalization\n",
    "- ✅ Learning rate scheduler\n",
    "- ✅ Visualización de métricas en tiempo real\n",
    "- ✅ Sistema robusto de guardado/carga de modelos\n",
    "- ✅ Listo para inferencia en imágenes nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# Configuración del dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración:\n",
      "  data_dir: objetos_salon/processed\n",
      "  batch_size: 64\n",
      "  train_split: 0.7\n",
      "  val_split: 0.15\n",
      "  test_split: 0.15\n",
      "  img_size: 32\n",
      "  num_workers: 2\n",
      "  latent_dim: 128\n",
      "  ae_epochs: 30\n",
      "  ae_lr: 0.001\n",
      "  ae_patience: 5\n",
      "  classifier_epochs: 25\n",
      "  classifier_lr: 0.001\n",
      "  classifier_patience: 7\n",
      "  dropout: 0.5\n",
      "  export_dir: export_v4\n",
      "  save_best_only: True\n"
     ]
    }
   ],
   "source": [
    "# Configuración\n",
    "CONFIG = {\n",
    "    # Datos\n",
    "    'data_dir': 'objetos_salon/processed',\n",
    "    'batch_size': 64,\n",
    "    'train_split': 0.7,\n",
    "    'val_split': 0.15,\n",
    "    'test_split': 0.15,\n",
    "    'img_size': 32,\n",
    "    'num_workers': 2,\n",
    "    \n",
    "    # Autoencoder\n",
    "    'latent_dim': 128,\n",
    "    'ae_epochs': 30,\n",
    "    'ae_lr': 1e-3,\n",
    "    'ae_patience': 5,\n",
    "    \n",
    "    # Classifier\n",
    "    'classifier_epochs': 25,\n",
    "    'classifier_lr': 1e-3,\n",
    "    'classifier_patience': 7,\n",
    "    'dropout': 0.5,\n",
    "    \n",
    "    # Output\n",
    "    'export_dir': 'export_v4',\n",
    "    'save_best_only': True\n",
    "}\n",
    "\n",
    "print(\"Configuración:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparación de datos con augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(config):\n",
    "    \"\"\"\n",
    "    Crea DataLoaders con train/val/test split y data augmentation.\n",
    "    \"\"\"\n",
    "    img_size = config['img_size']\n",
    "    \n",
    "    # Transformaciones para entrenamiento (con augmentation)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Transformaciones para validación/test (sin augmentation)\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    # Cargar dataset completo\n",
    "    full_dataset = datasets.ImageFolder(root=config['data_dir'], transform=eval_transform)\n",
    "    classes = full_dataset.classes\n",
    "    \n",
    "    print(f\"\\nClases encontradas: {classes}\")\n",
    "    print(f\"Total de imágenes: {len(full_dataset)}\")\n",
    "    \n",
    "    # Dividir en train/val/test\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(config['train_split'] * total_size)\n",
    "    val_size = int(config['val_split'] * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "    \n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size, test_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Aplicar transformaciones de augmentation solo al train set\n",
    "    train_dataset.dataset = copy.deepcopy(full_dataset)\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    \n",
    "    print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
    "    \n",
    "    # Crear DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        num_workers=config['num_workers']\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=config['num_workers']\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False, \n",
    "        num_workers=config['num_workers']\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clases encontradas: ['cpu', 'mesa', 'mouse', 'nada', 'pantalla', 'silla', 'teclado']\n",
      "Total de imágenes: 1619\n",
      "Train: 1133 | Val: 242 | Test: 244\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "train_loader, val_loader, test_loader, classes = get_data_loaders(CONFIG)\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arquitectura mejorada del Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autoencoder creado con latent_dim=128\n",
      "Parámetros totales: 297,683\n"
     ]
    }
   ],
   "source": [
    "class ImprovedAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoencoder Convolucional mejorado con Batch Normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super(ImprovedAutoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder con Batch Normalization\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input: 3 x 32 x 32\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> 16 x 16 x 16\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # -> 32 x 8 x 8\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)   # -> 64 x 4 x 4 = 1024\n",
    "        )\n",
    "        \n",
    "        # Bottleneck: 1024 -> latent_dim\n",
    "        self.to_latent = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, latent_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Expand: latent_dim -> 1024\n",
    "        self.from_latent = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64 * 4 * 4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),  # -> 32 x 8 x 8\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),  # -> 16 x 16 x 16\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(16, 3, kernel_size=2, stride=2),   # -> 3 x 32 x 32\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = encoded.view(encoded.size(0), -1)\n",
    "        latent_code = self.to_latent(encoded)\n",
    "        \n",
    "        # Decode\n",
    "        decoded = self.from_latent(latent_code)\n",
    "        decoded = decoded.view(decoded.size(0), 64, 4, 4)\n",
    "        reconstructed = self.decoder(decoded)\n",
    "        \n",
    "        return reconstructed, latent_code\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Método auxiliar para extraer solo el latent code.\"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        encoded = encoded.view(encoded.size(0), -1)\n",
    "        latent_code = self.to_latent(encoded)\n",
    "        return latent_code\n",
    "\n",
    "# Crear modelo\n",
    "autoencoder = ImprovedAutoencoder(latent_dim=CONFIG['latent_dim'])\n",
    "print(f\"\\nAutoencoder creado con latent_dim={CONFIG['latent_dim']}\")\n",
    "print(f\"Parámetros totales: {sum(p.numel() for p in autoencoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento del Autoencoder con validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stopping para detener el entrenamiento si no hay mejora.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "        \n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'  EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f'  Validación mejoró a {val_loss:.4f}')\n",
    "\n",
    "\n",
    "def train_autoencoder(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    Entrena el autoencoder con validación y early stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['ae_lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3\n",
    "    )\n",
    "    early_stopping = EarlyStopping(patience=config['ae_patience'])\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENTRENAMIENTO DEL AUTOENCODER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(config['ae_epochs']):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, _ = model(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                reconstructed, _ = model(images)\n",
    "                loss = criterion(reconstructed, images)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{config[\"ae_epochs\"]}] '\n",
    "              f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"\\nEarly stopping en epoch {epoch+1}\")\n",
    "            model.load_state_dict(early_stopping.best_model_state)\n",
    "            break\n",
    "    \n",
    "    # Cargar el mejor modelo\n",
    "    if early_stopping.best_model_state is not None:\n",
    "        model.load_state_dict(early_stopping.best_model_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENTRENAMIENTO DEL AUTOENCODER COMPLETADO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entrenar autoencoder\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ae_history \u001b[38;5;241m=\u001b[39m train_autoencoder(autoencoder, train_loader, val_loader, CONFIG)\n",
      "Cell \u001b[0;32mIn[7], line 39\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(model, train_loader, val_loader, config)\u001b[0m\n\u001b[1;32m     37\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     38\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mae_lr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 39\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m     40\u001b[0m     optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mae_patience\u001b[39m\u001b[38;5;124m'\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     44\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n",
      "\u001b[0;31mTypeError\u001b[0m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# Entrenar autoencoder\n",
    "ae_history = train_autoencoder(autoencoder, train_loader, val_loader, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de aprendizaje\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(ae_history['train_loss'], label='Train Loss')\n",
    "plt.plot(ae_history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Autoencoder - Curvas de Aprendizaje')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar reconstrucciones\n",
    "def imshow(img, title=\"\"):\n",
    "    \"\"\"Desnormalizar y mostrar imagen.\"\"\"\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "autoencoder.eval()\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images_gpu = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, _ = autoencoder(images_gpu)\n",
    "\n",
    "# Mostrar originales vs reconstruidas\n",
    "fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(np.transpose((images[i]/2 + 0.5).numpy(), (1, 2, 0)))\n",
    "    axes[0, i].set_title('Original')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(np.transpose((reconstructed[i].cpu()/2 + 0.5).numpy(), (1, 2, 0)))\n",
    "    axes[1, i].set_title('Reconstruida')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clasificador mejorado (BUG CORREGIDO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Clasificador que usa el autoencoder pre-entrenado.\n",
    "    CORRECCIÓN: Usa correctamente el latent_code del autoencoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, autoencoder, num_classes, dropout=0.5, freeze_encoder=True):\n",
    "        super(ImprovedClassifier, self).__init__()\n",
    "        self.autoencoder = autoencoder\n",
    "        self.latent_dim = autoencoder.latent_dim\n",
    "        \n",
    "        # Congelar autoencoder si se requiere\n",
    "        if freeze_encoder:\n",
    "            for param in self.autoencoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Capas de clasificación\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extraer latent code del autoencoder (CORRECCIÓN CRÍTICA)\n",
    "        with torch.no_grad():\n",
    "            latent_code = self.autoencoder.encode(x)\n",
    "        \n",
    "        # Clasificar\n",
    "        output = self.classifier(latent_code)\n",
    "        return output\n",
    "\n",
    "# Crear clasificador\n",
    "classifier = ImprovedClassifier(\n",
    "    autoencoder=autoencoder,\n",
    "    num_classes=num_classes,\n",
    "    dropout=CONFIG['dropout']\n",
    ")\n",
    "\n",
    "print(f\"\\nClasificador creado\")\n",
    "print(f\"Entrada: {CONFIG['latent_dim']} (latent dim)\")\n",
    "print(f\"Salida: {num_classes} clases\")\n",
    "print(f\"Parámetros entrenables: {sum(p.numel() for p in classifier.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, config):\n",
    "    \"\"\"\n",
    "    Entrena el clasificador con validación y early stopping.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=config['classifier_lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENTRENAMIENTO DEL CLASIFICADOR\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(config['classifier_epochs']):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{config[\"classifier_epochs\"]}] '\n",
    "              f'Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | '\n",
    "              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping basado en accuracy de validación\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "            print(f'  Mejor modelo guardado (Val Acc: {val_acc:.2f}%)')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  Paciencia: {patience_counter}/{config[\"classifier_patience\"]}')\n",
    "            \n",
    "            if patience_counter >= config['classifier_patience']:\n",
    "                print(f\"\\nEarly stopping en epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Cargar el mejor modelo\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ENTRENAMIENTO COMPLETADO - Mejor Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar clasificador\n",
    "clf_history = train_classifier(classifier, train_loader, val_loader, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de aprendizaje\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(clf_history['train_loss'], label='Train Loss')\n",
    "ax1.plot(clf_history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Clasificador - Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(clf_history['train_acc'], label='Train Acc')\n",
    "ax2.plot(clf_history['val_acc'], label='Val Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Clasificador - Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluación en test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, test_loader, classes):\n",
    "    \"\"\"\n",
    "    Evalúa el clasificador en el test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUACIÓN EN TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'\\nTest Accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nReporte de Clasificación:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=classes, digits=3))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.ylabel('Etiqueta Real')\n",
    "    plt.title('Matriz de Confusión - Test Set')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, all_preds, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar en test set\n",
    "test_acc, test_preds, test_labels, test_probs = evaluate_classifier(classifier, test_loader, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sistema robusto de guardado de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_complete_model(autoencoder, classifier, config, classes, history, export_dir='export_v4'):\n",
    "    \"\"\"\n",
    "    Guarda todo lo necesario para usar el modelo en producción.\n",
    "    \"\"\"\n",
    "    os.makedirs(export_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Guardar pesos del autoencoder\n",
    "    torch.save(autoencoder.state_dict(), f\"{export_dir}/autoencoder_state.pt\")\n",
    "    print(f\"✓ Autoencoder guardado: {export_dir}/autoencoder_state.pt\")\n",
    "    \n",
    "    # 2. Guardar pesos del clasificador\n",
    "    torch.save(classifier.state_dict(), f\"{export_dir}/classifier_state.pt\")\n",
    "    print(f\"✓ Clasificador guardado: {export_dir}/classifier_state.pt\")\n",
    "    \n",
    "    # 3. Guardar el modelo completo (arquitectura + pesos) - BACKUP\n",
    "    torch.save({\n",
    "        'autoencoder': autoencoder,\n",
    "        'classifier': classifier\n",
    "    }, f\"{export_dir}/complete_models.pt\")\n",
    "    print(f\"✓ Modelos completos guardados: {export_dir}/complete_models.pt\")\n",
    "    \n",
    "    # 4. Guardar metadatos CORRECTOS\n",
    "    metadata = {\n",
    "        'framework': 'pytorch',\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        \n",
    "        # Información de las clases\n",
    "        'classes': classes,\n",
    "        'num_classes': len(classes),\n",
    "        \n",
    "        # Arquitectura\n",
    "        'architecture': {\n",
    "            'latent_dim': config['latent_dim'],\n",
    "            'img_size': config['img_size'],\n",
    "            'dropout': config['dropout']\n",
    "        },\n",
    "        \n",
    "        # Transformaciones (CRÍTICO para inferencia)\n",
    "        'transforms': {\n",
    "            'resize': config['img_size'],\n",
    "            'normalize_mean': [0.5, 0.5, 0.5],\n",
    "            'normalize_std': [0.5, 0.5, 0.5]\n",
    "        },\n",
    "        \n",
    "        # Configuración de entrenamiento\n",
    "        'training': {\n",
    "            'ae_epochs': config['ae_epochs'],\n",
    "            'classifier_epochs': config['classifier_epochs'],\n",
    "            'batch_size': config['batch_size'],\n",
    "            'ae_lr': config['ae_lr'],\n",
    "            'classifier_lr': config['classifier_lr']\n",
    "        },\n",
    "        \n",
    "        # Métricas finales\n",
    "        'performance': {\n",
    "            'final_train_acc': history['train_acc'][-1] if history['train_acc'] else 0,\n",
    "            'final_val_acc': history['val_acc'][-1] if history['val_acc'] else 0,\n",
    "            'best_val_acc': max(history['val_acc']) if history['val_acc'] else 0\n",
    "        },\n",
    "        \n",
    "        # Información del sistema\n",
    "        'system': {\n",
    "            'device': str(device),\n",
    "            'pytorch_version': torch.__version__\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{export_dir}/metadata.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✓ Metadata guardado: {export_dir}/metadata.json\")\n",
    "    \n",
    "    # 5. Guardar historial de entrenamiento\n",
    "    with open(f\"{export_dir}/training_history.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"✓ Historial guardado: {export_dir}/training_history.json\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MODELO GUARDADO EXITOSAMENTE EN: {export_dir}/\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"\\nArchivos creados:\")\n",
    "    for file in os.listdir(export_dir):\n",
    "        filepath = os.path.join(export_dir, file)\n",
    "        size = os.path.getsize(filepath) / 1024  # KB\n",
    "        print(f\"  - {file} ({size:.1f} KB)\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar todo\n",
    "metadata = save_complete_model(\n",
    "    autoencoder=autoencoder,\n",
    "    classifier=classifier,\n",
    "    config=CONFIG,\n",
    "    classes=classes,\n",
    "    history=clf_history,\n",
    "    export_dir=CONFIG['export_dir']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Resumen del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN FINAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nClases: {classes}\")\n",
    "print(f\"Total clases: {len(classes)}\")\n",
    "print(f\"\\nAutoencoder:\")\n",
    "print(f\"  - Latent dim: {CONFIG['latent_dim']}\")\n",
    "print(f\"  - Mejor val loss: {min(ae_history['val_loss']):.4f}\")\n",
    "print(f\"\\nClasificador:\")\n",
    "print(f\"  - Mejor val accuracy: {max(clf_history['val_acc']):.2f}%\")\n",
    "print(f\"  - Test accuracy: {test_acc:.2f}%\")\n",
    "print(f\"\\nModelos guardados en: {CONFIG['export_dir']}/\")\n",
    "print(f\"\\nPróximo paso: Usar 'inference.py' para clasificar imágenes nuevas\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
