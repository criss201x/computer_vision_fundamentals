import cv2
import matplotlib.pyplot as plt
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch, torch.nn as nn, torch.nn.functional as F, math, numpy as np
import glob, os
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score


class ImageDataset(Dataset):
    def __init__(self, carpeta, size=224):
        self.paths = sorted(glob.glob(os.path.join(carpeta, "*.png")))
        self.size = size

    def letterbox(self, img):
        h, w = img.shape[:2]
        s = self.size / max(h, w)
        nh, nw = int(h * s), int(w * s)
        img_resized = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)
        pad_top = (self.size - nh) // 2
        pad_left = (self.size - nw) // 2
        out = np.full((self.size, self.size, 3), 114, dtype=np.uint8)
        out[pad_top:pad_top + nh, pad_left:pad_left + nw] = img_resized
        return out

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        img = cv2.imread(self.paths[idx], cv2.IMREAD_COLOR)
        img = self.letterbox(img)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.astype(np.float32) / 255.0
        img = torch.tensor(img).permute(2, 0, 1)  # C,H,W
        return img

dataset = ImageDataset("objetos_salon/processed/cpu", size=224)#ajustar tamaño despues
loader = DataLoader(dataset, batch_size=32, shuffle=True)


class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1), nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat


device = "cuda" if torch.cuda.is_available() else "cpu"
model = Autoencoder().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(10):
    total_loss = 0
    for imgs in loader:
        imgs = imgs.to(device)
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, imgs)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * imgs.size(0)
    print(f"Época {epoch+1}, pérdida promedio: {total_loss/len(dataset):.5f}")


model.eval()
imgs = next(iter(loader))
imgs = imgs[:5].to(device)
recon = model(imgs).detach().cpu()

for i in range(5):
    orig = imgs[i].cpu().permute(1,2,0).numpy()
    rec  = recon[i].permute(1,2,0).numpy()
    fig, ax = plt.subplots(1,2)
    ax[0].imshow(orig); ax[0].set_title("Original")
    ax[1].imshow(rec); ax[1].set_title("Reconstruida")
    plt.show()


#espacio latente 


# Supone que model = Autoencoder() ya está entrenado
model.eval()

# Toma un lote de imágenes
imgs = next(iter(loader))
imgs = imgs.to(device)

# Paso solo por el encoder
with torch.no_grad():
    z = model.encoder(imgs)

print("Forma de z:", z.shape)
#32 imágenes (batch).
#128 mapas de activación de tamaño 16×16 cada uno.


z_flat = z.view(z.size(0), -1).cpu().numpy()
print("Forma vectorizada:", z_flat.shape)


# reducción de dimensionalidad
z_pca = PCA(n_components=2).fit_transform(z_flat)
# o: z_tsne = TSNE(n_components=2, perplexity=30).fit_transform(z_flat)

plt.scatter(z_pca[:,0], z_pca[:,1], s=30, alpha=0.7)
plt.title("Representación 2D del espacio latente (PCA)")
plt.xlabel("Comp. 1")
plt.ylabel("Comp. 2")
plt.show()


# Tomamos una imagen y su z
x = imgs[0:1]
with torch.no_grad():
    z = model.encoder(x)

# Pequeña perturbación del vector latente
delta = torch.randn_like(z) * 0.1
x_mod = model.decoder(z + delta)

# Mostrar original, reconstruida y perturbada
def show_pair(a, b, title_a, title_b):
    fig, ax = plt.subplots(1,2, figsize=(5,3))
    ax[0].imshow(a); ax[0].set_title(title_a)
    ax[1].imshow(b); ax[1].set_title(title_b)
    plt.show()

a = x[0].detach().cpu().permute(1,2,0).numpy()
b = x_mod[0].detach().cpu().permute(1,2,0).numpy()
show_pair(a, b, "Original", "Reconstruida + perturbación")


# super clasificador 


class LabeledImageDataset(Dataset):
    def __init__(self, root, size=224):
        self.size = size
        self.samples = []
        self.classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root,d))])
        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}
        for c in self.classes:
            for p in glob.glob(os.path.join(root, c, "*.png")):
                self.samples.append((p, self.class_to_idx[c]))

    def letterbox(self, img):
        h, w = img.shape[:2]
        s = self.size / max(h, w)
        nh, nw = int(h*s), int(w*s)
        r = cv2.resize(img, (nw, nh), interpolation=cv2.INTER_AREA)
        out = np.full((self.size, self.size, 3), 114, dtype=np.uint8)
        y0 = (self.size - nh)//2; x0 = (self.size - nw)//2
        out[y0:y0+nh, x0:x0+nw] = r
        return out

    def __len__(self): return len(self.samples)

    def __getitem__(self, i):
        p, y = self.samples[i]
        img = cv2.imread(p, cv2.IMREAD_COLOR)
        img = cv2.cvtColor(self.letterbox(img), cv2.COLOR_BGR2RGB)
        x = torch.tensor(img, dtype=torch.float32).permute(2,0,1)/255.0
        return x, torch.tensor(y, dtype=torch.long)


ds = LabeledImageDataset("objetos_salon/processed", size=224)
n_classes = len(ds.classes)
loader = DataLoader(ds, batch_size=64, shuffle=True, num_workers=2)

encoder = model.encoder.eval()            # usa tu autoencoder entrenado
for p in encoder.parameters(): p.requires_grad = False
device = "cuda" if torch.cuda.is_available() else "cpu"
encoder.to(device)


# modo A


# Extraer features (flatten de z)
X, Y = [], []
with torch.no_grad():
    for x, y in loader:
        z = encoder(x.to(device))                 # [B, C, H, W]
        z = torch.nn.functional.adaptive_avg_pool2d(z, 1)  # GAP -> [B,C,1,1]
        z = z.view(z.size(0), -1).cpu().numpy()   # [B, C]
        X.append(z); Y.append(y.numpy())
X = np.concatenate(X, axis=0); Y = np.concatenate(Y, axis=0)

Xtr, Xte, Ytr, Yte = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)
clf = LogisticRegression(max_iter=2000).fit(Xtr, Ytr)
pred = clf.predict(Xte)
print("Acc:", accuracy_score(Yte, pred))
print(classification_report(Yte, pred, target_names=ds.classes))


# modo B



head = nn.Sequential(
    nn.AdaptiveAvgPool2d(1),             # -> [B,C,1,1]
    nn.Flatten(),                        # -> [B,C]
    nn.Linear(128, n_classes)            # ajusta "128" a canales de tu encoder
).to(device)

crit = nn.CrossEntropyLoss()
opt = optim.Adam(head.parameters(), lr=1e-3)

for epoch in range(20):
    head.train(); tot=0
    for x,y in loader:
        x,y = x.to(device), y.to(device)
        with torch.no_grad():
            z = encoder(x)
        logits = head(z)
        loss = crit(logits, y)
        opt.zero_grad(); loss.backward(); opt.step()
        tot += loss.item()*x.size(0)
    print(f"epoch {epoch+1} loss {tot/len(ds):.4f}")


# modo C


for name, p in encoder.named_parameters():
    p.requires_grad = name.startswith("2.")  # ejemplo: sólo último bloque
opt = optim.Adam(list(filter(lambda t: t.requires_grad, encoder.parameters())) + list(head.parameters()), lr=1e-4)


for epoch in range(20):
    head.train(); tot=0
    for x,y in loader:
        x,y = x.to(device), y.to(device)
        with torch.no_grad():
            z = encoder(x)
        logits = head(z)
        loss = crit(logits, y)
        opt.zero_grad(); loss.backward(); opt.step()
        tot += loss.item()*x.size(0)
    print(f"epoch {epoch+1} loss {tot/len(ds):.4f}")


# experimento con diferentes tamaños de cuello


class AutoencoderK(nn.Module):
    def __init__(self, k=64):
        super().__init__()
        self.enc_base = nn.Sequential(
            nn.Conv2d(3, 32, 3, 2, 1), nn.ReLU(),
            nn.Conv2d(32,64, 3, 2, 1), nn.ReLU(),
            nn.Conv2d(64,128,3, 2, 1), nn.ReLU(),   # -> [B,128,16,16] para 224x224
        )
        self.enc_neck = nn.Conv2d(128, k, kernel_size=1)       # -> [B,k,16,16]
        self.dec_neck = nn.Conv2d(k, 128, kernel_size=1)       # <- [B,128,16,16]
        self.dec_base = nn.Sequential(
            nn.ConvTranspose2d(128,64,4,2,1), nn.ReLU(),
            nn.ConvTranspose2d(64,32,4,2,1),  nn.ReLU(),
            nn.ConvTranspose2d(32,3, 4,2,1),  nn.Sigmoid()
        )

    def encode(self, x):
        h = self.enc_base(x)
        return self.enc_neck(h)               # z: [B,k,16,16]

    def decode(self, z):
        h = self.dec_neck(z)
        return self.dec_base(h)

    def forward(self, x):
        z = self.encode(x)
        xhat = self.decode(z)
        return xhat, z


def train_ae(model, loader, device, epochs=10, lr=1e-3):
    model.to(device)
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    for ep in range(epochs):
        model.train(); tot = 0
        for batch in loader:
            x = batch if isinstance(batch, torch.Tensor) else batch[0]
            x = x.to(device)
            opt.zero_grad()
            xhat, _ = model(x)
            loss = F.mse_loss(xhat, x)
            loss.backward(); opt.step()
            tot += loss.item() * x.size(0)
        print(f"época {ep+1:02d}  MSE {tot/len(loader.dataset):.6f}")


def eval_recon(model, loader, device, batches=5):
    model.eval(); mse_sum=0; n=0
    with torch.no_grad():
        for i, batch in enumerate(loader):
            x = batch if isinstance(batch, torch.Tensor) else batch[0]
            x = x.to(device)
            xhat, _ = model(x)
            mse = F.mse_loss(xhat, x).item()
            mse_sum += mse; n += 1
            if i+1 >= batches: break
    mse = mse_sum/n
    psnr = 10*math.log10(1.0/mse)
    return mse, psnr


def linear_probe(model, loader, device, class_names):
    X, Y = [], []
    model.eval()
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            _, z = model(x)                                  # [B,k,16,16]
            z = F.adaptive_avg_pool2d(z, 1).view(z.size(0), -1).cpu().numpy()
            X.append(z); Y.append(y.numpy())
    X = np.concatenate(X); Y = np.concatenate(Y)
    Xtr, Xte, Ytr, Yte = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)
    clf = LogisticRegression(max_iter=2000).fit(Xtr, Ytr)
    acc = accuracy_score(Yte, clf.predict(Xte))
    return acc


device = "cuda" if torch.cuda.is_available() else "cpu"
k = 64
model = AutoencoderK(k=k)

# Entrenamiento (10 épocas sugeridas)
train_ae(model, loader, device, epochs=10, lr=1e-3)

# Reconstrucción: MSE y PSNR
mse, psnr = eval_recon(model, loader, device, batches=8)
print(f"MSE recon: {mse:.6f}   PSNR: {psnr:.2f} dB")


acc = linear_probe(model, loader, device, class_names=getattr(ds, "classes", None))
print(f"Linear probe (k=64) accuracy: {acc:.4f}")


# k = 128 a ver como nos va 


class ImageDatasetAE(Dataset):
    def __init__(self, carpeta, size=224, aug=True):
        self.paths = sorted(glob.glob(os.path.join(carpeta, "*.png")))
        self.size, self.aug = size, aug

    def letterbox(self, img):
        h,w = img.shape[:2]; s = self.size/max(h,w)
        nh,nw = int(h*s), int(w*s)
        r = cv2.resize(img,(nw,nh), interpolation=cv2.INTER_AREA)
        out = np.full((self.size,self.size,3),114,np.uint8)
        y0=(self.size-nh)//2; x0=(self.size-nw)//2
        out[y0:y0+nh, x0:x0+nw] = r
        return out

    def augment(self, img):
        if np.random.rand()<0.5: img = img[:, ::-1]                 # flip H
        if np.random.rand()<0.3:                                     # brillo
            g = 0.9 + 0.2*np.random.rand()
            img = np.clip(img.astype(np.float32)*g,0,255).astype(np.uint8)
        return img

    def __len__(self): return len(self.paths)

    def __getitem__(self, i):
        img = cv2.imread(self.paths[i], cv2.IMREAD_COLOR)
        img = self.letterbox(img)
        if self.aug: img = self.augment(img)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0
        x = torch.from_numpy(img).permute(2,0,1)     # [C,H,W] en [0,1]
        return x

ds_ae = ImageDatasetAE("objetos_salon/processed/cpu", size=224, aug=True)
loader_ae = DataLoader(ds_ae, batch_size=64, shuffle=True, num_workers=2)


class AutoencoderK(nn.Module):
    def __init__(self, k=128):
        super().__init__()
        self.enc_base = nn.Sequential(
            nn.Conv2d(3, 32, 3, 2, 1), nn.ReLU(),
            nn.Conv2d(32,64, 3, 2, 1), nn.ReLU(),
            nn.Conv2d(64,128,3, 2, 1), nn.ReLU(),   # -> [B,128,16,16]
        )
        self.enc_neck = nn.Conv2d(128, k, 1)        # -> [B,k,16,16]
        self.dec_neck = nn.Conv2d(k, 128, 1)
        self.dec_base = nn.Sequential(
            nn.ConvTranspose2d(128,64,4,2,1), nn.ReLU(),
            nn.ConvTranspose2d(64,32,4,2,1),  nn.ReLU(),
            nn.ConvTranspose2d(32,3, 4,2,1),  nn.Sigmoid()
        )

    def encode(self, x):  return self.enc_neck(self.enc_base(x))
    def decode(self, z):  return self.dec_base(self.dec_neck(z))
    def forward(self, x): z = self.encode(x); return self.decode(z), z


device = "cuda" if torch.cuda.is_available() else "cpu"
model = AutoencoderK(k=128).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3)
sched = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=3, verbose=True)

def train_ae(model, loader, epochs=30):
    for ep in range(1, epochs+1):
        model.train(); tot=0
        for x in loader:
            x = x.to(device)
            opt.zero_grad()
            xhat, _ = model(x)
            loss = F.mse_loss(xhat, x)
            loss.backward(); opt.step()
            tot += loss.item()*x.size(0)
        avg = tot/len(loader.dataset)
        sched.step(avg)
        print(f"época {ep:02d}  MSE {avg:.6f}")

train_ae(model, loader_ae, epochs=30)


def eval_recon(model, loader, batches=8):
    model.eval(); mse_sum=0; n=0
    with torch.no_grad():
        for i,x in enumerate(loader):
            x = x.to(device)
            xhat,_ = model(x)
            mse = F.mse_loss(xhat, x).item()
            mse_sum += mse; n += 1
            if i+1>=batches: break
    mse = mse_sum/n
    psnr = 10*math.log10(1.0/mse)
    return mse, psnr

mse, psnr = eval_recon(model, loader_ae)
print(f"MSE recon: {mse:.6f}   PSNR: {psnr:.2f} dB")


# su dataset etiquetado:
ds_clf = LabeledImageDataset("objetos_salon/processed", size=224)
loader_clf = DataLoader(ds_clf, batch_size=64, shuffle=False, num_workers=2)

def linear_probe(model, loader):
    X,Y = [],[]
    model.eval()
    with torch.no_grad():
        for x,y in loader:
            x = x.to(device)
            _, z = model(x)                               # [B,k,16,16]
            z = F.adaptive_avg_pool2d(z,1).view(z.size(0), -1).cpu().numpy()
            X.append(z); Y.append(y.numpy())
    X = np.concatenate(X); Y = np.concatenate(Y)
    Xtr,Xte,Ytr,Yte = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=42)
    clf = LogisticRegression(max_iter=2000).fit(Xtr,Ytr)
    acc = accuracy_score(Yte, clf.predict(Xte))
    return acc

acc = linear_probe(model, loader_clf)
print(f"Linear probe (k=128) accuracy: {acc:.4f}")



